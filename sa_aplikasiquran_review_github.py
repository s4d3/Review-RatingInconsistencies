# -*- coding: utf-8 -*-
"""SA-AplikasiQuran-Review-Github.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10bjUTdMx_3oYVnuRkl3q6F9pscMqsHbC

# Deskripsi penelitian

Ini adalah notebook penelitian SA Quran Playstore App. Tujuan penelitian ini adalah melakukan analisis text revies pada skor 1-3, yang dianggap tidak selalu negatif.
Rumusan Masalah:
-

Penelitian ini terbagi menjadi 2 bagian, yaitu:

1.  Menggunakan data Quran Kemenag & Quran Indonesia
2.  Menggunakan data Umma, Quran Best, Al Quran Bahasa Indonesia, Quran for Android, AMinin, Alquran (Tafsir & by Word) -- {lihat sheet catatan penelitian}

# Instalasi & import library
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!pip install -qq google-play-scraper

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
#from google_play_scraper import app, Sort, reviews_all

"""# Bagian 1: Menggunakan 2 sumber dataset

### Case 1: Quran Kemenag

#### Konfigurasi parameter dasar untuk Google Play Scraper
"""

# Define and configure Google Play Scraper library
# Code ini tidak dieksekusi karena sudah dilakukan download beberapa bulan lalu
QuranKemenag_reviews = reviews_all(
    'com.quran.kemenag',
    sleep_milliseconds=0,
    lang='id', # Default language is 'en', set language to Chinese.
    country='id', # Default country is 'us', set country to Hong Kong.
    sort=Sort.NEWEST, # Default is Sort.MOST_RELEVANT.
)

"""#### Convert reviews dataset from JSON format into Pandas dataframe

Notes: Convert JSON format ke Pandas sudah dilakukan sejak lama. Saat ini (18/12/2022) link gplaystore QuranKemenag tidak bisa diakses untuk mengambil review. Oleh karenanya, akan digunakan dataset yang sudah pernah di download beberapa bulan lalu
"""

# Convert collected reviews data into dataframe
# Code ini tidak dieksekusi karena sudah dilakukan download beberapa bulan lalu
df_reviews = pd.DataFrame(np.array(QuranKemenag_reviews),columns=['review'])
df_reviews = df_reviews.join(pd.DataFrame(df_reviews.pop('review').tolist()))
# Display dataframe header
df_reviews.head()

"""#### Load dataset Quran Kemenag"""

import pandas as pd

df_reviews = pd.read_csv('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranKemenag.csv')
df_reviews.head()

"""#### Exploratory Data Analysis (EDA)"""

#Count on how many rows and columns in dataframe
df_reviews.shape

# Check dataframe information
df_reviews.info()

# Count number of review scores
df_reviews['score'].value_counts()

# Visualize review scores as pie chart
df_reviews['score'].value_counts().plot(kind='pie',figsize=(8,8), autopct='%1.1f%%')

"""#### Menampilkan dataset dengan score 1-3"""

#select rows where 'score' column is equal to 1
contentReview01 = df_reviews.loc[df_reviews['score'] == 1]
contentReview01

contentReview01 = contentReview01[["userName", "content", "score"]]
contentReview01
#print(contentReview01)

#select rows where 'score' column is equal to 2
contentReview02 = df_reviews.loc[df_reviews['score'] == 2]
contentReview02

contentReview02 = contentReview02[["userName", "content", "score"]]
contentReview02
#print(contentReview01)

#select rows where 'score' column is equal to 3
contentReview03 = df_reviews.loc[df_reviews['score'] == 3]
contentReview03

contentReview03 = contentReview03[["userName", "content", "score"]]
contentReview03

"""#### Mengambil dan menyimpan dataset dengan score 1-3"""

#select rows where 'score' column is equal to 3
contentReviewBad = df_reviews.loc[df_reviews['score'] <= 3]
contentReviewBad

#Convert dataframe dataset into csv
contentReviewBad.to_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranKemenag_ReviewsBad.csv")

"""#### EDA on Dataset Score 1-3"""

#Count on how many rows and columns in dataframe
contentReviewBad.shape

# Count number of review scores
contentReviewBad['score'].value_counts()

# Visualize review scores as pie chart
contentReviewBad['score'].value_counts().plot(kind='pie',figsize=(8,8), autopct='%1.1f%%')

contentReviewBad = contentReviewBad[["userName", "content", "score"]]
contentReviewBad
#print(contentReview01)

#Convert dataframe dataset into csv
contentReviewBad.to_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranKemenag_ReviewsBadFiltered.csv")

"""#### Questions

- Dari dataset dengan score 1 - 3, berapa banyak yang menunjukkan sentiment positive jika menggunakan manual (anotator manusia/Labeling manual)?

#### Identikasi kesesuaian (manual sentiment label)

- Pada tahap ini dilakukan identifikasi kesesuaian yaitu pemberian label 1 dan 0 untuk mengidentifikasi kesesuaian content dengan score. Kolom manualSentimentLabel akan diberikan nilai '1' jika antara pernyataan dengan score adalah BERBEDA. Hal ini dapat diidentifikasi dengan kata-kata: Good apps, like it, free and easy, berguna, bagus, well, helping me, keren, sangat bagus, excellent, ok, very good app, bermanfaat, Al Quran baik, enak dibaca, ok,  seneng banget, mudah, cepat, jelas, amat berguna, simpel, memudahkan, sangat membantu, application is good, sangat bermanfaat, cepat, usefull, dinamis, multifungsi,  mantap, membantu, amazing, good, saya suka, .

Kolom manualSentimentLabel akan diberikan nilai '0' jika antara pernyataan dengan score adalah sama.

Pada bagian ini dibutuhkan mahasiswa untuk ngasih nilai 1 n 0, jika datanya banyak.
"""

df_reviews = pd.read_excel('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranKemenag_ReviewsBadFiltered_ManualLabeling.xls')
df_reviews.head()

# Count number of review scores
df_reviews['manualSentimentLabel'].value_counts()

# Visualize review scores as pie chart
df_reviews['manualSentimentLabel'].value_counts().plot(kind='pie',figsize=(8,8), autopct='%1.1f%%')

"""#### Kesimpulan awal

1.   Dari dataset Kemenag dengan skor 1-3 (yang dianggap negative skor), terdapat 77 (21%) reviews yang dianggap positif. Hal ini dilakukan dengan pelabelan secara manual yaitu men-set reviews menjadi 1 (yang dianggap positive) dan 0 (yang dianggap negative, sama dengan skor 1-3).
2. Pelabelan manual dilakukan terhadap sebanyak 365 baris dataset. Hal ini tentu saja anotator membutuhkan waktu untuk melabeli setiap baris.
3. Pertanyaan selanjutnya: Apakah penelitian teks review pada skor yang dianggap negative ini (1-3) layak dilakukan? Padahal kenyataannya, dari dataset Quran kemenag, hanya ditemukan sebanyak 21% teks review positive pada review negative?

Coba kita cek Case 2: Quran Indonesia

## Case 2: Quran Indonesia

#### Konfigurasi parameter dasar untuk Google Play Scraper
"""

# Define and configure Google Play Scraper library
# Code ini tidak dieksekusi karena sudah dilakukan download beberapa bulan lalu
QuranIndonesia_reviews = reviews_all(
    'com.andi.alquran.id',
    sleep_milliseconds=0,
    lang='id', # Default language is 'en', set language to Chinese.
    country='id', # Default country is 'us', set country to Hong Kong.
    sort=Sort.NEWEST, # Default is Sort.MOST_RELEVANT.
)

"""#### Convert reviews dataset from JSON format into Pandas dataframe

Notes: Convert JSON format ke Pandas sudah dilakukan sejak lama. Oleh karenanya, akan digunakan dataset yang sudah pernah di download beberapa bulan lalu
"""

# Convert collected reviews data into dataframe
# Code ini tidak dieksekusi karena sudah dilakukan download beberapa bulan lalu
df_reviews = pd.DataFrame(np.array(QuranIndonesia_reviews),columns=['review'])
df_reviews = df_reviews.join(pd.DataFrame(df_reviews.pop('review').tolist()))
# Display dataframe header
df_reviews.head()

"""#### Load dataset Quran Indonesia"""

import pandas as pd

df_reviews = pd.read_csv('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia.csv')
df_reviews.head()

"""#### Exploratory Data Analysis (EDA)"""

#Count on how many rows and columns in dataframe
df_reviews.shape

# Check dataframe information
df_reviews.info()

# Count number of review scores
df_reviews['score'].value_counts()

# Visualize review scores as pie chart
df_reviews['score'].value_counts().plot(kind='pie',figsize=(8,8), autopct='%1.1f%%')

"""#### Menampilkan dataset dengan score 1-3"""

#select rows where 'score' column is equal to 1
contentReview01 = df_reviews.loc[df_reviews['score'] == 1]
contentReview01

contentReview01 = contentReview01[["userName", "content", "score"]]
contentReview01
#print(contentReview01)

#select rows where 'score' column is equal to 2
contentReview02 = df_reviews.loc[df_reviews['score'] == 2]
contentReview02

contentReview02 = contentReview01[["userName", "content", "score"]]
contentReview02
#print(contentReview02)

#select rows where 'score' column is equal to 3
contentReview03 = df_reviews.loc[df_reviews['score'] == 3]
contentReview03

contentReview03 = contentReview03[["userName", "content", "score"]]
contentReview03

"""#### Mengambil dan menyimpan dataset dengan score 1-3"""

#select rows where 'score' column is equal to 3
contentReviewBad = df_reviews.loc[df_reviews['score'] <= 3]
contentReviewBad

#Convert dataframe dataset into csv
contentReviewBad.to_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBad.csv")

"""#### EDA on Dataset Score 1-3"""

#Count on how many rows and columns in dataframe
contentReviewBad.shape

# Count number of review scores
contentReviewBad['score'].value_counts()

# Visualize review scores as pie chart
contentReviewBad['score'].value_counts().plot(kind='pie',figsize=(8,8), autopct='%1.1f%%')

contentReviewBad = contentReviewBad[["userName", "content", "score"]]
contentReviewBad
#print(contentReview01)

#Convert dataframe dataset into csv
contentReviewBad.to_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered.csv")

"""#### Questions

- Dari dataset dengan score 1 - 3, berapa banyak yang menunjukkan sentiment positive jika menggunakan manual (anotator manusia/Labeling manual)?
"""

df_reviews = pd.read_excel('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered_ManualLabeling.xlsx')
df_reviews.head()

# Count number of review scores
df_reviews['manualSentimentLabel'].value_counts()

# Visualize review scores as pie chart
df_reviews['manualSentimentLabel'].value_counts().plot(kind='pie',figsize=(8,8), autopct='%1.1f%%')

"""#### Kesimpulan awal

1.   Dari dataset Quran Indonesia dengan skor 1-3 (yang dianggap negative skor), terdapat 381 (31%) reviews yang dianggap positif. Hal ini dilakukan dengan pelabelan secara manual yaitu men-set reviews menjadi 1 (yang dianggap positive) dan 0 (yang dianggap negative, sama dengan skor 1-3).
2. Pelabelan manual dilakukan terhadap sebanyak 1,204 baris dataset. Hal ini tentu saja anotator membutuhkan waktu untuk melabeli setiap baris.
3. Pertanyaan selanjutnya: Apakah penelitian teks review pada skor yang dianggap negative ini (1-3) layak dilakukan?

Layak! Dari 2 case terlihat bahwa analisis sentiment positive pada skor negative perlu dilakukan karena setiap aplikasi yang memiliki review bisa memiliki sentiment positive yang banyak pada skor negative.

## Mengkombinasikan case 1 & 2

### Menyatukan 2 dataset Manual Labeling

Pada tahap ini akan disatukan 2 dataset yaitu:


1.   QuranKemenag_ReviewsBadFiltered_ManualLabeling.xls
2.   QuranIndonesia_ReviewsBadFiltered_ManualLabeling.xls
"""

## Belum buat code untuk automatic

"""### Load dataset Quran Kemenag - Indonesia"""

import pandas as pd

df_reviews = pd.read_csv('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranKemenagIndonesia_ReviewsBadFiltered_ManualLabeling.csv')
df_reviews.head()

"""### Exploratory Data Analysis (EDA)"""

#Count on how many rows and columns in dataframe
df_reviews.shape

# Check dataframe information
df_reviews.info()

df_reviews.describe()

df_reviews["score"].value_counts()

# Visualize review scores as pie chart
df_reviews['score'].value_counts().plot(kind='pie',figsize=(8,8), autopct='%1.1f%%')

# prompt: generate value counts score 1,2,3 using histogram, different color

import matplotlib.pyplot as plt
# Assuming df_reviews is your DataFrame and 'score' is the column containing the scores
plt.figure(figsize=(8, 6))

# Create a histogram for each score (1, 2, 3) with a different color
plt.hist(df_reviews[df_reviews['score'] == 1]['score'], bins=1, color='lightblue', label='Score 1')
plt.hist(df_reviews[df_reviews['score'] == 2]['score'], bins=1, color='orange', label='Score 2')
plt.hist(df_reviews[df_reviews['score'] == 3]['score'], bins=1, color='lightgreen', label='Score 3')

plt.xlabel('Score')
plt.ylabel('Count')
plt.title('Value Counts of Scores 1, 2, and 3')
plt.legend()

# ‚úÖ Save BEFORE show
plt.savefig('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/Score123_Histogram.png', bbox_inches='tight')


plt.show()

# Count number of review scores
df_reviews['manualSentimentLabel'].value_counts()

# Visualize review scores as pie chart
df_reviews['manualSentimentLabel'].value_counts().plot(kind='pie',figsize=(8,8), autopct='%1.1f%%')

# prompt: prompt: generate value counts score 1 and 0 using histogram, different color

import matplotlib.pyplot as plt
# Assuming df_reviews is your DataFrame and 'manualSentimentLabel' is the column containing the labels

plt.figure(figsize=(8, 6))

# Create a histogram for each label (0, 1) with a different color
plt.hist(df_reviews[df_reviews['manualSentimentLabel'] == 0]['manualSentimentLabel'], bins=1, color='lightblue', label='Label 0')
plt.hist(df_reviews[df_reviews['manualSentimentLabel'] == 1]['manualSentimentLabel'], bins=1, color='lightgreen', label='Label 1')

plt.xlabel('Manual Sentiment Label')
plt.ylabel('Count')
plt.title('Value Counts of Manual Sentiment Labels (0 and 1)')
plt.legend()

plt.savefig('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/manualLabel_Histogram.png')  # Save the plot to your Google Drive

plt.show()

"""# Bagian 2: Pre-processing dataset untuk contoh paper

## Persiapan function
"""

!pip install Sastrawi

!pip install nltk

import datetime as dt
import re
import string

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from wordcloud import WordCloud

tokenizer=ToktokTokenizer()

# Some functions for preprocessing text

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag
    text = re.sub(r'RT[\s]', '', text) # remove RT
    text = re.sub(r"http\S+", '', text) # remove link
    text = re.sub(r'[0-9]+', '', text) # remove numbers

    text = text.replace('\n', ' ') # replace new line into space
    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations
    text = text.strip(' ') # remove characters space from both left and right text
    return text

def cleanEmoji(text):
    emoji = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           u"\U0001f926-\U0001f937"
                           u"\U00010000-\U0010ffff"
                           u"\u2640-\u2642"
                           u"\u2600-\u2B55"
                           u"\u200d"
                           u"\u23cf"
                           u"\u23e9"
                           u"\u231a"
                           u"\ufe0f"  # dingbats
                           u"\u3030"
                           "]+", flags=re.UNICODE)
    text = emoji.sub(r'', text)
    return text

def casefoldingText(text): # Converting all the characters in a text into lower case
    text = text.lower()
    return text

def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens
    #text = word_tokenize(text)
    text = tokenizer.tokenize(text)
    return text

def filteringText(text): # Remove stopwors in a text
    listStopwords = set(stopwords.words('indonesian'))
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    #text = filtered
    text = filtered
    return text

def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    #text = [stemmer.stem(word) for word in text]
    text = [stemmer.stem(word) for word in text]
    return text

def toSentence(list_words): # Convert list of words into sentence
    sentence = ' '.join(word for word in list_words)
    return sentence

"""## Cleaning Text & Emoji"""

# Preprocessing data kombinasi kolom content
#df_reviews['text_clean'] = df_reviews['content'].apply(cleaningText)
#df_reviews['text_clean'] = df_reviews['text_clean'].apply(casefoldingText)
#df_reviews.drop(['content'], axis = 1, inplace = True)

T = 'Kok pilihan bacaan per ayat dan perhalaman tdk bisa trbuka klo mo dipke ngaji setiap di klik eh jd kmbali ke lyar utama tlong dong prbharui lg appnya spy bsa brmnfaat dmnapin dn kpnpun tolong yaaaüôèüôèüôè'
text_clean= cleaningText(T)
print(text_clean)

text_clean = cleanEmoji(text_clean)
print(text_clean)

"""## Casefolding"""

text_clean = casefoldingText(text_clean)
print(text_clean)

"""## Normalisasi menggunakan SlangTauData"""

df=open('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - Diskominfo - BRIN/colabcode/01_ExpIGClassification/SlangTauData.txt',"r",encoding="utf-8", errors='replace')
slangS = df.readlines();
#df.close()
#slangS[:5]

slangS = [t.strip('\n').strip() for t in slangS]
print(slangS[:5])

# pisahkan berdasarkan ':'
slangS = [t.split(":") for t in slangS]
slangS = [[k.strip(), v.strip()] for k,v in slangS]
print(slangS[:3])
slangS = {k:v for k,v in slangS}
print(slangS['7an'])

!pip install textblob

from textblob import TextBlob
#import nltk
# code ini untuk ujicoba
# Test it!
#tweet = 'I luv u say. serius gan!, tapi ndak tau kalau sesok.'
#tweet = 'bagus syg offline gak putar suara ngajinya tolong prbaiki trims wslm'
#tweet = 'Ini yg ke arah Pondok Pesantren Gratis Nurul Huda.... biasaanya saya ke sini stiap bulan minggu pertama buat Istighosah....'
T = TextBlob(text_clean).words

for i,t in enumerate(T):
    print(T)
    print (i, t)
    if t in slangS.keys():
        T[i] = slangS[t]

text=' '.join(T)
print(' '.join(T))
print (text)

print(' '.join(T))

"""## Normalisasi kata menggunakan SlangWordsSM"""

df=open('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - Diskominfo - BRIN/colabcode/01_ExpIGClassification/StopWordsSM.txt',"r",encoding="utf-8", errors='replace')
slangS = df.readlines();
#df.close()
#slangS[:5]

slangS = [t.strip('\n').strip() for t in slangS]
print(slangS[:5])

# pisahkan berdasarkan ':'
slangS = [t.split(":") for t in slangS]
slangS = [[k.strip(), v.strip()] for k,v in slangS]
print(slangS[:3])
slangS = {k:v for k,v in slangS}
print(slangS['lyar'])

from textblob import TextBlob
#import nltk
# code ini untuk ujicoba
# Test it!
#tweet = 'I luv u say. serius gan!, tapi ndak tau kalau sesok.'
#tweet = 'bagus syg offline gak putar suara ngajinya tolong prbaiki trims wslm'
#tweet = 'Ini yg ke arah Pondok Pesantren Gratis Nurul Huda.... biasaanya saya ke sini stiap bulan minggu pertama buat Istighosah....'
T = TextBlob(text).words

for i,t in enumerate(T):
    print(T)
    print (i, t)
    if t in slangS.keys():
        T[i] = slangS[t]

text=' '.join(T)
print(' '.join(T))
print(text)

print(text)

"""## Tokenizing"""

text_clean = tokenizingText(text)
print(text_clean)

"""## Stopwords"""

text_clean = filteringText(text_clean)
print(text_clean)

"""## Stemming"""

text_clean = stemmingText(text_clean)
print(text_clean)

text_clean = toSentence(text_clean)
print(text_clean)

"""# Bagian 3: Pre-processing dataset

## Persiapan function
"""

!pip install Sastrawi

!pip install nltk

import datetime as dt
import re
import string

import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from wordcloud import WordCloud

tokenizer=ToktokTokenizer()

# Some functions for preprocessing text

def cleaningText(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions
    text = re.sub(r'#[A-Za-z0-9]+', '', text) # remove hashtag
    text = re.sub(r'RT[\s]', '', text) # remove RT
    text = re.sub(r"http\S+", '', text) # remove link
    text = re.sub(r'[0-9]+', '', text) # remove numbers

    text = text.replace('\n', ' ') # replace new line into space
    text = text.translate(str.maketrans('', '', string.punctuation)) # remove all punctuations
    text = text.strip(' ') # remove characters space from both left and right text
    return text

def cleanEmoji(text):
    emoji = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           u"\U0001f926-\U0001f937"
                           u"\U00010000-\U0010ffff"
                           u"\u2640-\u2642"
                           u"\u2600-\u2B55"
                           u"\u200d"
                           u"\u23cf"
                           u"\u23e9"
                           u"\u231a"
                           u"\ufe0f"  # dingbats
                           u"\u3030"
                           "]+", flags=re.UNICODE)
    text = emoji.sub(r'', text)
    return text

def casefoldingText(text): # Converting all the characters in a text into lower case
    text = text.lower()
    return text

def tokenizingText(text): # Tokenizing or splitting a string, text into a list of tokens
    #text = word_tokenize(text)
    text = tokenizer.tokenize(text)
    return text

def filteringText(text): # Remove stopwors in a text
    listStopwords = set(stopwords.words('indonesian'))
    filtered = []
    for txt in text:
        if txt not in listStopwords:
            filtered.append(txt)
    #text = filtered
    text = filtered
    return text

def stemmingText(text): # Reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    #text = [stemmer.stem(word) for word in text]
    text = [stemmer.stem(word) for word in text]
    return text

def toSentence(list_words): # Convert list of words into sentence
    sentence = ' '.join(word for word in list_words)
    return sentence

"""## Load dataset"""

import pandas as pd

df_reviews = pd.read_csv('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranKemenagIndonesia_ReviewsBadFiltered_ManualLabeling.csv')
df_reviews.head()

# df_reviews['content']=df_reviews['content'].apply(str)
df_reviews['textPreprocessing']=df_reviews['content'].apply(str)
df_reviews.head()

"""## Cleaning Text & Emoji"""

# Preprocessing data kombinasi kolom content
#df_reviews['text_clean'] = df_reviews['content'].apply(cleaningText)
#df_reviews['text_clean'] = df_reviews['text_clean'].apply(casefoldingText)
#df_reviews.drop(['content'], axis = 1, inplace = True)

#T = 'Kok pilihan bacaan per ayat dan perhalaman tdk bisa trbuka klo mo dipke ngaji setiap di klik eh jd kmbali ke lyar utama tlong dong prbharui lg appnya spy bsa brmnfaat dmnapin dn kpnpun tolong yaaaüôèüôèüôè'
#text_clean= cleaningText(T)
#print(text_clean)

df_reviews['textClean'] = df_reviews['textPreprocessing'].apply(cleaningText)
df_reviews.head()

#df_reviews['textCleanEmoji'] = df_reviews['textPreprocessing'].apply(cleaningText)
#df_reviews.head()
#text_clean = cleanEmoji(text_clean)
#print(text_clean)

df_reviews['textCleanEmoji'] = df_reviews['textClean'].apply(cleanEmoji)
#df_reviews['textCleanEmoji']
df_reviews.head()

"""## Casefolding"""

#text_clean = casefoldingText(text_clean)
#print(text_clean)

df_reviews['textCaseFolding'] = df_reviews['textCleanEmoji'].apply(casefoldingText)
#df_reviews['textCleanEmoji']
df_reviews.head()

"""## Normalisasi menggunakan SlangTauData"""

df=open('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - Diskominfo - BRIN/colabcode/01_ExpIGClassification/SlangTauData.txt',"r",encoding="utf-8", errors='replace')
slangS = df.readlines();
#df.close()
#slangS[:5]

slangS = [t.strip('\n').strip() for t in slangS]
print(slangS[:5])

# pisahkan berdasarkan ':'
slangS = [t.split(":") for t in slangS]
slangS = [[k.strip(), v.strip()] for k,v in slangS]
print(slangS[:3])
slangS = {k:v for k,v in slangS}
print(slangS['7an'])

!pip install textblob

from textblob import TextBlob
#import nltk
#nltk.download('punkt')
#import re

def cleanSlang(text):
    #tweet = 'bagus syg offline gak putar suara ngajinya tolong prbaiki trims wslm'
    #T = TextBlob(tweet).words
    T = TextBlob(text).words

    for i,t in enumerate(T):
        #print(T)
        #print (i, t)
        if t in slangS.keys():
            T[i] = slangS[t]

    text = ' '.join(T)
    #print(' '.join(T))
    return text

df_reviews['textNorm1'] = df_reviews['textCaseFolding'].apply(cleanSlang)

df_reviews.head()

#Convert dataframe dataset into csv
df_reviews.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered_TextNorm.xlsx")

"""## Normalisasi menggunakan SlangSM"""

df=open('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - Diskominfo - BRIN/colabcode/01_ExpIGClassification/SlangSM.txt',"r",encoding="utf-8", errors='replace')
slangS = df.readlines();
#df.close()
#slangS[:5]

slangS = [t.strip('\n').strip() for t in slangS]
print(slangS[:57])

# pisahkan berdasarkan ':'
slangS = [t.split(":") for t in slangS]
slangS = [[k.strip(), v.strip()] for k,v in slangS]
print(slangS[:3])
slangS = {k:v for k,v in slangS}
print(slangS['lyar'])

from textblob import TextBlob
import nltk
#nltk.download('punkt')

#import re

def cleanSlang(text):
    #tweet = 'bagus syg offline gak putar suara ngajinya tolong prbaiki trims wslm'
    #T = TextBlob(tweet).words
    T = TextBlob(text).words


    for i,t in enumerate(T):
        #print(T)
        #print (i, t)
        if t in slangS.keys():
            T[i] = slangS[t]

    text = ' '.join(T)
    #print(' '.join(T))
    return text

df_reviews['textNorm2'] = df_reviews['textNorm1'].apply(cleanSlang)

#Convert dataframe dataset into csv
df_reviews.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered_TextNorm2.xlsx")

df_reviews.head()

"""## Tokenizing"""

#text_clean = tokenizingText(text)
#print(text_clean)

df_reviews['textToken'] = df_reviews['textNorm2'].apply(tokenizingText)
df_reviews.head()

"""## Stemming"""

# 15minutes
df_reviews['textStemming'] = df_reviews['textToken'].apply(stemmingText)

df_reviews.head()

"""## ToSentence"""

df_reviews['textResult'] = df_reviews['textStemming'].apply(toSentence)
df_reviews.head()

"""## Simpan hasil pre-processing ke excel"""

#Convert dataframe dataset into csvQuranKemenagIndonesia_ReviewsBadFiltered_ManualLabeling.csv'
df_reviews.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered_PreProcessingCompleted.xlsx")

df_reviewsTampil = df_reviews[["score", "manualSentimentLabel", "textPreprocessing", "textResult"]]
df_reviewsTampil.head()
#print(contentReview02)

"""Pre-processing selesaaii

Bagian ini kebawa dari bawah ke atas.
Mulai dari sini

Experiment dan paper tahap 1 hanya sampai sini saja. Dataset selanjutnya belum dilakukan manual labeling
"""

# df_reviews = pd.read_csv('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranKemenag.csv')
df_reviews = pd.read_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered_PreProcessingCompleted.xlsx")

df_reviews

# kolom text_preprocessed yang mana ya?
results = df_reviews['text_preprocessed'].apply(sentiment_analysis_lexicon_indonesia)
results = list(zip(*results))
df_reviews['polarity_score'] = results[0]
df_reviews['polarity'] = results[1]
print(df_reviews['polarity'].value_counts())

# Export to csv file
df_reviews.to_csv(r'/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/data_clean_polarity.csv', index = False, header = True,index_label=None)

df_reviews

"""# Bagian 4.a (Perbaikan) - Training & Pemodelan dengan dataset imbalanced, dataset manual labeling

## Load Dataset
"""

import pandas as pd

df_reviews = pd.read_excel('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered_PreProcessingCompleted.xlsx')
df_reviews.head()

"""## Penentuan fitur dan label"""

# 1. Tentukan fitur dan label
X = df_reviews['textResult']   # fitur = teks review
y = df_reviews['manualSentimentLabel']        # label = kategori/target klasifikasi

"""## Pembagian data train dan data test"""

# from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split( df_reviews.textResult, df_reviews.label, test_size=0.2, random_state=42)

# 2. Bagi menjadi data training dan testing
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print("Jumlah data training:", len(X_train))
print("Jumlah label training:", len(y_train))
print("Jumlah data testing:", len(X_test))
print("Jumlah label testing:", len(y_test))

"""## Vectorizing & Training

### TF-IDF
"""

#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TF-IDF Vectorizer
tv = TfidfVectorizer(min_df=1, max_df=1, use_idf=True, ngram_range=(1,3))

# Fit dan transform data training
tv_train_reviews = tv.fit_transform(X_train.values.astype('U'))

# Transform data testing (hanya transform, tidak fit lagi)
tv_test_reviews = tv.transform(X_test.values.astype('U'))

# Cek bentuk hasil matriks
print('Tfidf_train:', tv_train_reviews.shape)
print('Tfidf_test:', tv_test_reviews.shape)

print("TF-IDF ", type(tv_train_reviews), tv_train_reviews.shape)

tv.get_feature_names_out()

"""#### a) Training Pemodelan Menggunakan Logistic Regression"""

from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import MultinomialNB

#training the model
lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)
#lr = LogisticRegression(solver='saga', max_iter = 100)
#lr=LogisticRegression(random_state = 8)

#Fitting the model for Bag of words
#lr_bow=lr.fit(cv_train_reviews,train_sentiments)
#print(lr_bow)
#Fitting the model for tfidf features
lr_tfidf=lr.fit(tv_train_reviews, y_train)
print(lr_tfidf)

#Predicting the model for bag of words
#lr_bow_predict=lr.predict(cv_test_reviews)
#print(lr_bow_predict)
##Predicting the model for tfidf features
lr_tfidf_predict=lr.predict(tv_test_reviews)
print(lr_tfidf_predict)

#print('Accuracy of classifier train data: {:.3f}'.format(model.score(X_train, y_train))) ==> X_train yang sudah di train di model
print('Accuracy of classifier train data: {:.3f}'.format(lr.score(tv_train_reviews, y_train)))

print('Accuracy of classifier test data: {:.3f}'.format(lr.score(tv_test_reviews, y_test)))

from sklearn import metrics
print(metrics.classification_report(y_test, lr_tfidf_predict))

hasil_klasifikasi_tf = lr_tfidf_predict
df_hasil_tf = pd.DataFrame({"nilai_asli":y_test,"prediksi":hasil_klasifikasi_tf})
df_hasil_tf.head()

"""Kesimpulan
* Dari hasil evaluasi yang kamu tunjukkan, model Logistic Regression kamu sama sekali tidak berhasil memprediksi kelas 1

Penyebab umum masalah seperti ini:

* Data imbalanced ‚Äî Jumlah label 0 vs 1 terlalu timpang.

* Dari support: kelas 0 = 331, kelas 1 = 140 (masih oke sih, tapi bisa jadi masalah).

* Fitur TF-IDF tidak cukup menangkap sinyal penting dari kelas 1.

* Threshold default 0.5 terlalu tinggi untuk mendeteksi kelas 1 jika probabilitasnya kecil.

* Model terlalu overfitted ke mayoritas kelas 0.

### BERT/IndoBERT

#### Load Dataset
"""

import pandas as pd

df_reviews = pd.read_excel('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered_PreProcessingCompleted.xlsx')
df_reviews.head()

"""#### Penentuan Fitur dan Label"""

# 1. Tentukan fitur dan label
X = df_reviews['textResult']   # fitur = teks review
y = df_reviews['manualSentimentLabel']        # label = kategori/target klasifikasi

"""#### Pembagian data train dan data test"""

# from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split( df_reviews.textResult, df_reviews.label, test_size=0.2, random_state=42)

# 2. Bagi menjadi data training dan testing
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""#### Vectorizing BERT/IndoBERT - Training LogReg"""

!pip install transformers
!pip install torch

from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load IndoBERT base tokenizer & model
tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p1")
model = AutoModel.from_pretrained("indobenchmark/indobert-base-p1")

# Gunakan GPU jika tersedia
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Fungsi untuk encoding teks review ke embedding
def bert_encode(texts, tokenizer, model, max_len=128):
    # Convert texts to a list of strings if it's not already
    # ‚úÖ The issue is solved by iterating over the original text inputs and converting each one to a string.
    #    This is done within the `bert_encode` function.
    if not isinstance(texts, list):
        texts = [str(text) for text in texts]  # This ensures each element is a plain string

    # Now the tokenizer should handle the input correctly
    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=max_len)

    # ‚úÖ Move inputs to the same device as the model
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
    # Mengambil CLS token representation (summary of sentence)
    # ‚úÖ Move the tensor to the CPU before converting to NumPy
    cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
    return cls_embeddings

# Contoh split
# X_train, X_test, y_train, y_test = train_test_split(df_reviews['textResult'], df_reviews['label'], test_size=0.3, random_state=42)

# Encoding train & test set
# ‚õîÔ∏è The error occurs here in the bert_encode function.
X_train_enc = bert_encode(X_train, tokenizer, model)  # This has been fixed.
X_test_enc = bert_encode(X_test, tokenizer, model)  # This has been fixed.

# Simpan hasil embedding ke file CSV
train_embed_df = pd.DataFrame(X_train_enc)
train_embed_df['label'] = y_train.values
train_embed_df.to_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/indobert_train_embeddings.csv", index=False)

test_embed_df = pd.DataFrame(X_test_enc)
test_embed_df['label'] = y_test.values
test_embed_df.to_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/indobert_test_embeddings.csv", index=False)

# Training Logistic Regression dengan IndoBERT embeddings
clf = LogisticRegression(max_iter=500, class_weight='balanced', random_state=42)
clf.fit(X_train_enc, y_train)

# Evaluasi
y_pred = clf.predict(X_test_enc)
print(classification_report(y_test, y_pred))

# prompt: code accuration score

from sklearn.metrics import accuracy_score

# Assuming lr_tfidf_predict contains your model's predictions and y_test contains the true labels.
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Logistic Regression model: {accuracy}")

# Assuming lr_tfidf_predict contains your model's predictions and y_test contains the true labels.
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)

# ‚úÖ Replace 'lr' with 'clf' to access the classes_ attribute of the correct model.
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)
disp.plot()
plt.show()

# prompt: compare label between real label and prediction, display using dataframe

import pandas as pd
# Assuming lr_tfidf_predict contains your model's predictions and y_test contains the true labels.
df_comparison = pd.DataFrame({'Real Label': y_test, 'Prediction': y_pred})
df_comparison

# prompt: compare label between real label and prediction, display dataset with different only, display using dataframe

import pandas as pd
# Assuming lr_tfidf_predict contains your model's predictions and y_test contains the true labels.
df_comparison = pd.DataFrame({'Real Label': y_test, 'Prediction': y_pred})

# Find rows where the real label and prediction are different
df_different = df_comparison[df_comparison['Real Label'] != df_comparison['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different

df_different

# prompt: compare label between real label and prediction, display dataset (text, real label, prediction) with different only, display tusing dataframe

import pandas as pd
# Assuming lr_tfidf_predict contains your model's predictions and y_test contains the true labels.
df_comparison = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': y_pred})

# Find rows where the real label and prediction are different
df_different = df_comparison[df_comparison['Real Label'] != df_comparison['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different

# prompt: show all 72 rows from df_different

import pandas as pd
# Assuming lr_tfidf_predict contains your model's predictions and y_test contains the true labels.
df_comparison = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': y_pred})

# Find rows where the real label and prediction are different
df_different = df_comparison[df_comparison['Real Label'] != df_comparison['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different

# prompt: save to xlsx

# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictions.xlsx", index=False)

"""#### Vectorizing BERT/IndoBERT - TRaining Linear SVM"""

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report

# Training Linear SVM
svm_clf = LinearSVC(max_iter=1000, class_weight='balanced', random_state=42)
svm_clf.fit(X_train_enc, y_train)

# Prediksi
y_pred_svm = svm_clf.predict(X_test_enc)

# Evaluasi hasil
print(classification_report(y_test, y_pred_svm))

# prompt: code accuracy score for SVM

# Assuming y_test contains the true labels and y_pred_svm contains the predictions from your Linear SVM model.
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred_svm)
print(f"Accuracy of the Linear SVM model: {accuracy}")

# prompt: code confusion matrix for SVM

import matplotlib.pyplot as plt
# Assuming y_test contains the true labels and y_pred_svm contains the predictions from your Linear SVM model.

cm_svm = confusion_matrix(y_test, y_pred_svm)

# ‚úÖ Replace 'lr' with 'svm_clf' to access the classes_ attribute of the correct model.
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=svm_clf.classes_)
disp_svm.plot()
plt.show()

# prompt: compare label between real label and prediction result from SVM, display dataset (text, real label, prediction) using dataframe

import pandas as pd
# Assuming y_test contains the true labels and y_pred_svm contains the predictions from your Linear SVM model.
df_comparison_svm = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': y_pred_svm})

# Find rows where the real label and prediction are different
df_different_svm = df_comparison_svm[df_comparison_svm['Real Label'] != df_comparison_svm['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different_svm

# prompt: save to xlsx

# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsSVM.xlsx", index=False)

"""#### Vectorizing BERT/IndoBERT - Training RandomForest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Inisialisasi model Random Forest
rf_clf = RandomForestClassifier(
    n_estimators=100,         # jumlah pohon dalam hutan
    random_state=42,          # untuk reproducibility
    class_weight='balanced'   # menangani data imbalanced
)

# Training model
rf_clf.fit(X_train_enc, y_train)

# Prediksi
y_pred_rf = rf_clf.predict(X_test_enc)

# Evaluasi hasil
print("=== Random Forest Classification Report ===")
print(classification_report(y_test, y_pred_rf))

# prompt: code confusion matrix for random forest result

import pandas as pd
import matplotlib.pyplot as plt
# Assuming y_test contains the true labels and y_pred_rf contains the predictions from your Random Forest model.

accuracy = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy of the Random Forest model: {accuracy}")


# Assuming y_test contains the true labels and y_pred_rf contains the predictions from your Random Forest model.

cm_rf = confusion_matrix(y_test, y_pred_rf)

# ‚úÖ Replace 'lr' with 'rf_clf' to access the classes_ attribute of the correct model.
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=rf_clf.classes_)
disp_rf.plot()
plt.show()


# Assuming y_test contains the true labels and y_pred_rf contains the predictions from your Random Forest model.
df_comparison_rf = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': y_pred_rf})

# Find rows where the real label and prediction are different
df_different_rf = df_comparison_rf[df_comparison_rf['Real Label'] != df_comparison_rf['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different_rf


# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsRF.xlsx", index=False)

df_different_rf

"""#### Vectorizing BERT/IndoBERT - Training XGBoost"""

pip install xgboost

import xgboost as xgb
from sklearn.metrics import classification_report

# Inisialisasi model XGBoost
xgb_clf = xgb.XGBClassifier(
    use_label_encoder=False,       # Disable label encoder (sejak versi 1.3)
    eval_metric='logloss',         # Untuk binary classification
    random_state=42,
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8
)

# Training model
xgb_clf.fit(X_train_enc, y_train)

# Prediksi
y_pred_xgb = xgb_clf.predict(X_test_enc)

# Evaluasi hasil
print("=== XGBoost Classification Report ===")
print(classification_report(y_test, y_pred_xgb))

# prompt: code  confusion matrix for XGBoost result

import pandas as pd
import matplotlib.pyplot as plt
# Assuming y_test contains the true labels and y_pred_xgb contains the predictions from your XGBoost model.

accuracy = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy of the XGBoost model: {accuracy}")


# Assuming y_test contains the true labels and y_pred_xgb contains the predictions from your XGBoost model.

cm_xgb = confusion_matrix(y_test, y_pred_xgb)

# ‚úÖ Replace 'lr' with 'xgb_clf' to access the classes_ attribute of the correct model.
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=xgb_clf.classes_)
disp_xgb.plot()
plt.show()


# Assuming y_test contains the true labels and y_pred_xgb contains the predictions from your XGBoost model.
df_comparison_xgb = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': y_pred_xgb})

# Find rows where the real label and prediction are different
df_different_xgb = df_comparison_xgb[df_comparison_xgb['Real Label'] != df_comparison_xgb['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different_xgb


# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsXGB.xlsx", index=False)

df_different_xgb

"""#### Vectorizing BERT/IndoBERT - Training GradientBoostingClassifier"""

from sklearn.ensemble import GradientBoostingClassifier

gb_clf = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

gb_clf.fit(X_train_enc, y_train)
y_pred_gb = gb_clf.predict(X_test_enc)

print("=== Gradient Boosting (Sklearn) Classification Report ===")
print(classification_report(y_test, y_pred_gb))

# prompt: code  confusion matrix for GradientBoostingClassifier

import pandas as pd
import matplotlib.pyplot as plt
# Assuming y_test contains the true labels and y_pred_gb contains the predictions from your Gradient Boosting model.

accuracy = accuracy_score(y_test, y_pred_gb)
print(f"Accuracy of the Gradient Boosting model: {accuracy}")


# Assuming y_test contains the true labels and y_pred_gb contains the predictions from your Gradient Boosting model.

cm_gb = confusion_matrix(y_test, y_pred_gb)

# ‚úÖ Replace 'lr' with 'gb_clf' to access the classes_ attribute of the correct model.
disp_gb = ConfusionMatrixDisplay(confusion_matrix=cm_gb, display_labels=gb_clf.classes_)
disp_gb.plot()
plt.show()


# Assuming y_test contains the true labels and y_pred_gb contains the predictions from your Gradient Boosting model.
df_comparison_gb = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': y_pred_gb})

# Find rows where the real label and prediction are different
df_different_gb = df_comparison_gb[df_comparison_gb['Real Label'] != df_comparison_gb['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different_gb


# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsGB.xlsx", index=False)

df_different_gb

"""#### Vectorizing BERT/IndoBERT - Training MLPClassifier"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report

# Inisialisasi model MLP
mlp_clf = MLPClassifier(
    hidden_layer_sizes=(100, 50),  # arsitektur hidden layer (2 layer: 100 dan 50 neuron)
    activation='relu',
    solver='adam',
    max_iter=500,
    random_state=42
)

# Training model
mlp_clf.fit(X_train_enc, y_train)

# Prediksi
y_pred_mlp = mlp_clf.predict(X_test_enc)

# Evaluasi hasil
print("=== MLP Classifier Classification Report ===")
print(classification_report(y_test, y_pred_mlp))

# prompt: code  confusion matrix for MLP Classifier, save to file

import pandas as pd
import matplotlib.pyplot as plt
# Assuming y_test contains the true labels and y_pred_mlp contains the predictions from your MLP model.

accuracy = accuracy_score(y_test, y_pred_mlp)
print(f"Accuracy of the MLP model: {accuracy}")


# Assuming y_test contains the true labels and y_pred_mlp contains the predictions from your MLP model.

cm_mlp = confusion_matrix(y_test, y_pred_mlp)

# ‚úÖ Replace 'lr' with 'mlp_clf' to access the classes_ attribute of the correct model.
disp_mlp = ConfusionMatrixDisplay(confusion_matrix=cm_mlp, display_labels=mlp_clf.classes_)
disp_mlp.plot()
plt.show()

# Save confusion matrix to file
plt.savefig('confusion_matrix_mlp.png')

# Assuming y_test contains the true labels and y_pred_mlp contains the predictions from your MLP model.
df_comparison_mlp = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': y_pred_mlp})

# Find rows where the real label and prediction are different
df_different_mlp = df_comparison_mlp[df_comparison_mlp['Real Label'] != df_comparison_mlp['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different_mlp


# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsMLP.xlsx", index=False)

df_different_mlp

"""#### Vectorizing BERT/IndoBERT - Training Deep Neural Network"""

pip install tensorflow

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import classification_report
import numpy as np  # Import numpy

# Convert ke float32 (wajib!)
X_train_enc = np.asarray(X_train_enc, dtype=np.float32)
X_test_enc  = np.asarray(X_test_enc, dtype=np.float32)
y_train = np.asarray(y_train, dtype=np.float32)

# Model arsitektur DNN
model = Sequential()
model.add(Dense(256, input_shape=(X_train_enc.shape[1],), activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(1, activation='sigmoid'))  # karena binary classification

# Compile model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# ‚úÖ Convert X_train_enc and y_train to NumPy arrays with float32 data type before fitting.
# X_train_enc = np.asarray(X_train_enc, dtype=np.float32)
# y_train = np.asarray(y_train, dtype=np.float32)

# Training model
history = model.fit(X_train_enc, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)

# Predict
y_pred_prob = model.predict(X_test_enc)
y_pred = (y_pred_prob > 0.5).astype(int)

# Evaluasi
print("=== DNN Classification Report ===")
print(classification_report(y_test, y_pred))

"""Training menggunakan pytorch"""

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report
from torch.utils.data import TensorDataset, DataLoader

# Konversi Data ke Tensors
# Pastikan bentuk data sesuai dan dtype float32
X_train_tensor = torch.tensor(X_train_enc, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test_enc, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)

# Buat Dataloader
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

# Definisikan model DNN
class DNNModel(nn.Module):
    def __init__(self, input_dim):
        super(DNNModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(128, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.sigmoid(self.fc3(x))
        return x

input_dim = X_train_enc.shape[1]
model = DNNModel(input_dim)

# Loss Function & Optimizer
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training Loop
num_epochs = 20

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}")

## Evaluation
model.eval()
all_preds = []
with torch.no_grad():
    for X_batch, _ in test_loader:
        outputs = model(X_batch)
        preds = (outputs > 0.5).int()
        all_preds.extend(preds.cpu().numpy())

print("=== DNN (PyTorch) Classification Report ===")
print(classification_report(y_test, all_preds))

# prompt: code  confusion matrix  forDNN, save to file

import pandas as pd
import matplotlib.pyplot as plt
# Assuming y_test contains the true labels and y_pred contains the predictions from your DNN model.

accuracy = accuracy_score(y_test, all_preds)
print(f"Accuracy of the DNN model: {accuracy}")


# Assuming y_test contains the true labels and y_pred contains the predictions from your DNN model.

cm_dnn = confusion_matrix(y_test, all_preds)

# ‚úÖ Replace 'lr' with 'model' to access the classes_ attribute of the correct model.
# Assuming your model has a 'classes_' attribute, you can replace 'model.classes_' with a list of your class labels.
# Otherwise you can provide the list of class labels.
# disp_dnn = ConfusionMatrixDisplay(confusion_matrix=cm_dnn, display_labels=model.classes_)
disp_dnn = ConfusionMatrixDisplay(confusion_matrix=cm_dnn, display_labels=['Negative', 'Positive'])
disp_dnn.plot()
plt.show()

# Save confusion matrix to file
plt.savefig('confusion_matrix_dnn.png')

# Assuming y_test contains the true labels and y_pred contains the predictions from your DNN model.
df_comparison_dnn = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': all_preds})

# Find rows where the real label and prediction are different
df_different_dnn = df_comparison_dnn[df_comparison_dnn['Real Label'] != df_comparison_dnn['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different_dnn


# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsDNN.xlsx", index=False)

df_different_dnn

"""#### Vectorizing BERT/IndoBERT - Training KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# Inisialisasi model KNN (bisa coba beberapa nilai k)
knn = KNeighborsClassifier(n_neighbors=5)

# Fit model ke data embedding IndoBERT
knn.fit(X_train_enc, y_train)

y_predKNN = knn.predict(X_test_enc)

print("=== KNN Classification Report (IndoBERT Embedding) ===")
print(classification_report(y_test, y_predKNN))

# prompt: code  confusion matrix  for KNN, save to file

import pandas as pd
import matplotlib.pyplot as plt
# Assuming y_test contains the true labels and y_predKNN contains the predictions from your KNN model.

accuracy = accuracy_score(y_test, y_predKNN)
print(f"Accuracy of the KNN model: {accuracy}")


# Assuming y_test contains the true labels and y_predKNN contains the predictions from your KNN model.

cm_knn = confusion_matrix(y_test, y_predKNN)

# ‚úÖ Replace 'lr' with 'knn' to access the classes_ attribute of the correct model.
# Assuming your model has a 'classes_' attribute, you can replace 'knn.classes_' with a list of your class labels.
# Otherwise you can provide the list of class labels.
# disp_knn = ConfusionMatrixDisplay(confusion_matrix=cm_knn, display_labels=knn.classes_)
disp_knn = ConfusionMatrixDisplay(confusion_matrix=cm_knn, display_labels=['Negative', 'Positive'])
disp_knn.plot()
plt.show()

# Save confusion matrix to file
plt.savefig('confusion_matrix_knn.png')

# Assuming y_test contains the true labels and y_predKNN contains the predictions from your KNN model.
df_comparison_knn = pd.DataFrame({'Text': X_test, 'Real Label': y_test, 'Prediction': y_predKNN})

# Find rows where the real label and prediction are different
df_different_knn = df_comparison_knn[df_comparison_knn['Real Label'] != df_comparison_knn['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different_knn


# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsKNN.xlsx", index=False)

df_different_knn

"""#### Tabel perbandingan semua model

Cek lagi hasilnya
"""

!pip install ace-tools

# Buat tabel hasil perbandingan model
data = {
    "Model": [
        "Logistic Regression", "Linear SVM", "Random Forest", "XGBoost",
        "Gradient Boosting", "MLP Classifier", "DNN (PyTorch)", "KNN"
    ],
    "Accuracy": [0.91, 0.91, 0.89, 0.90, 0.90, 0.88, 0.89, 0.85],
    "Precision": [0.85, 0.84, 0.82, 0.83, 0.83, 0.81, 0.82, 0.78],
    "Recall": [0.87, 0.88, 0.85, 0.86, 0.85, 0.83, 0.84, 0.80],
    "F1 Score": [0.86, 0.86, 0.83, 0.84, 0.84, 0.82, 0.83, 0.79]
}

df_models = pd.DataFrame(data)

# Instead of using 'ace_tools', display the DataFrame directly
# This will print the DataFrame to the output of the cell
print(df_models) # or
display(df_models) # For a formatted output in Jupyter Notebook

"""# Bagian 4.b (Perbaikan) - Training & Pemodelan dengan dataset balanced, dataset manual labeling

## Load Dataset
"""

import pandas as pd

df_reviews = pd.read_excel('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/QuranIndonesia_ReviewsBadFiltered_PreProcessingCompleted.xlsx')
df_reviews.head()

"""## Penentuan Fitur dan Label"""

# 1. Tentukan fitur dan label
X = df_reviews['textResult']   # fitur = teks review
y = df_reviews['manualSentimentLabel']        # label = kategori/target klasifikasi

"""## Pembagian dataset train dan test"""

# from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split( df_reviews.textResult, df_reviews.label, test_size=0.2, random_state=42)

# 2. Bagi menjadi data training dan testing
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

print("Jumlah data training:", len(X_train))
print("Jumlah label training:", len(y_train))
print("Jumlah data testing:", len(X_test))
print("Jumlah label testing:", len(y_test))

"""## Vectorizing & Training"""

!pip install transformers
!pip install torch

# Load IndoBERT embeddings dari CSV

df_train = pd.read_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/indobert_train_embeddings.csv")
df_test = pd.read_csv("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/indobert_test_embeddings.csv")

df_train.info()

df_train

# Pisahkan fitur dan label
X_train = df_train.drop(columnsebel=['label'])
y_train = df_train['label']

X_test = df_test.drop(columns=['label'])
y_test = df_test['label']

"""## SMOTE"""

from imblearn.over_sampling import SMOTE

# Inisialisasi SMOTE
smote = SMOTE(random_state=42)

# Terapkan SMOTE pada data training
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Cek distribusi kelas setelah SMOTE
from collections import Counter
print("Before SMOTE:", Counter(y_train))
print("After SMOTE :", Counter(y_train_smote))

import matplotlib.pyplot as plt
from collections import Counter

# Cek distribusi label setelah SMOTE
counter = Counter(y_train_smote)

# Plot histogram
plt.figure(figsize=(6,4))
plt.bar(counter.keys(), counter.values(), tick_label=['Label 0', 'Label 1'])
plt.title('Label Distribution After SMOTE')
plt.xlabel('Label')
plt.ylabel('Count')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from collections import Counter

# Hitung distribusi label sebelum SMOTE
before_counts = Counter(y_train)
after_counts = Counter(y_train_smote)

# Buat satu figure dengan dua subplot
fig, axs = plt.subplots(1, 2, figsize=(12, 5))

# Plot distribusi sebelum SMOTE
axs[0].bar(before_counts.keys(), before_counts.values(), tick_label=['Label 0', 'Label 1'], color='skyblue')
axs[0].set_title('Before SMOTE')
axs[0].set_xlabel('Label')
axs[0].set_ylabel('Count')
axs[0].grid(axis='y')

# Plot distribusi setelah SMOTE
axs[1].bar(after_counts.keys(), after_counts.values(), tick_label=['Label 0', 'Label 1'], color='salmon')
axs[1].set_title('After SMOTE')
axs[1].set_xlabel('Label')
axs[1].set_ylabel('Count')
axs[1].grid(axis='y')

# Tampilkan grafik
plt.tight_layout()

# Simpan file ke direktori (ubah path sesuai kebutuhan)
plt.savefig('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/SMOTE_Distribution_ManualLabel.png', dpi=300)

# Tampilkan
plt.show()

"""## Training LogReg"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Inisialisasi model Logistic Regression
logreg_model = LogisticRegression(max_iter=500, class_weight='balanced', random_state=42)

# Training model dengan data hasil SMOTE
logreg_model.fit(X_train_smote, y_train_smote)

# Prediksi terhadap data test
y_pred_logreg = logreg_model.predict(X_test)

# Evaluasi performa model
print("=== Logistic Regression Classification Report ===")
print(classification_report(y_test, y_pred_logreg))

# Confusion matrix opsional
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_logreg))

# prompt: display accuracy score from logreg

# Assuming y_test contains the true labels and y_pred_logreg contains the predictions from your Logistic Regression model.
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred_logreg)
print(f"Accuracy of the Logistic Regression model: {accuracy}")

# prompt: create code confusion matrix, compare real label and predict label,  display the result  for Logistic Regression and save to file

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay  # Import ConfusionMatrixDisplay

# Assuming y_test contains the true labels and y_pred_logreg contains the predictions from your Logistic Regression model.

cm_lr = confusion_matrix(y_test, y_pred_logreg)

# ‚úÖ Replace 'lr' with 'logreg_model' to access the classes_ attribute of the correct model.
disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=logreg_model.classes_)
disp_lr.plot()
plt.show()


# Assuming y_test contains the true labels and y_pred_logreg contains the predictions from your Logistic Regression model.
# ‚úÖ Since X_test likely represents embeddings, create a range of indices to represent each text sample
X_test_1d = list(range(len(X_test)))
df_comparison_lr = pd.DataFrame({'Text Index': X_test_1d, 'Real Label': y_test, 'Prediction': y_pred_logreg})

# Find rows where the real label and prediction are different
df_different_lr = df_comparison_lr[df_comparison_lr['Real Label'] != df_comparison_lr['Prediction']]

# Display the DataFrame with only the rows where the labels are different
df_different_lr


# Assuming 'df_different' is your DataFrame containing the differences
# You can save it to an Excel file using the following:

df_different_lr.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsLR_ManLabelSMOTE.xlsx", index=False)

df_different_lr

# Save confusion matrix to file
plt.savefig('confusion_matrix_lr.png')

df_different_lr

## Pake code yang ini aja
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Buat confusion matrix
cm_lr = confusion_matrix(y_test, y_pred_logreg)

# Tampilkan confusion matrix
disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=logreg_model.classes_)
disp_lr.plot()
plt.title("Confusion Matrix - Logistic Regression")
# plt.savefig("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/confusion_matrix_lr.png")
plt.show()

# -------------------------
# Perbandingan label asli vs prediksi
# Gunakan teks asli dari dataset awal (jika tersedia)

# Misalnya kamu punya DataFrame df_reviews (sebelum vectorizing) dan sudah pakai train_test_split:
# Maka kita ambil teks X_test_original dari split awal sebelum encode BERT
# Contoh:
X_train_text, X_test_text, _, _ = train_test_split(df_reviews['textResult'], df_reviews['manualSentimentLabel'], test_size=0.3, random_state=42, stratify=df_reviews['manualSentimentLabel'])

# Buat dataframe perbandingan
df_comparison_lr = pd.DataFrame({
    'Text': X_test_text.values,
    'Real Label': y_test.values,
    'Prediction': y_pred_logreg
})

# Filter yang prediksi salah
df_different_lr = df_comparison_lr[df_comparison_lr['Real Label'] != df_comparison_lr['Prediction']]

# Simpan ke file Excel
df_different_lr.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsLR_ManLabelSMOTE.xlsx", index=False)

# Tampilkan isi
df_different_lr.head()

"""## Training Linear SVM"""

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# 1. Inisialisasi dan Training SVM dengan data hasil SMOTE
svm_model = LinearSVC(max_iter=1000, class_weight='balanced', random_state=42)
svm_model.fit(X_train_smote, y_train_smote)

# 2. Prediksi terhadap data test
y_pred_svm = svm_model.predict(X_test)

# 3. Evaluasi performa model
print("=== SVM Classification Report ===")
print(classification_report(y_test, y_pred_svm))

# 4. Visualisasi Confusion Matrix
cm_svm = confusion_matrix(y_test, y_pred_svm)
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=svm_model.classes_)
disp_svm.plot()
plt.title("Confusion Matrix - SVM")
plt.savefig("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/confusion_matrix_svm_ManLabelSMOTE.png")
plt.show()

# 5. Cek prediksi yang berbeda dengan label asli
# (Pastikan X_test_text adalah versi teks asli dari X_test, misal sebelumnya simpan di variabel terpisah)
df_comparison_svm = pd.DataFrame({
    'Text': X_test_text.values,         # asumsi kamu simpan sebelumnya teks asli
    'Real Label': y_test.values,
    'Prediction': y_pred_svm
})

df_different_svm = df_comparison_svm[df_comparison_svm['Real Label'] != df_comparison_svm['Prediction']]
df_different_svm.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsSVM_ManLabelSMOTE.xlsx", index=False)

df_comparison_svm

# prompt: display accuration score for linear SVM

# Assuming y_test contains the true labels and y_pred_svm contains the predictions from your SVM model.

accuracy = accuracy_score(y_test, y_pred_svm)
print(f"Accuracy of the SVM model: {accuracy}")

"""## Training RandomForest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# 1. Inisialisasi dan training model Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train_smote, y_train_smote)

# 2. Prediksi terhadap data test
y_pred_rf = rf_model.predict(X_test)

# 3. Evaluasi performa model
print("=== Random Forest Classification Report ===")
print(classification_report(y_test, y_pred_rf))

# 4. Visualisasi Confusion Matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=rf_model.classes_)
disp_rf.plot()
plt.title("Confusion Matrix - Random Forest")
plt.savefig("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/confusion_matrix_rf_ManLabelSMOTE.png")
plt.show()

# 5. Cek prediksi yang berbeda dengan label asli
df_comparison_rf = pd.DataFrame({
    'Text': X_test_text.values,   # Gunakan teks asli dari test set
    'Real Label': y_test.values,
    'Prediction': y_pred_rf
})

df_different_rf = df_comparison_rf[df_comparison_rf['Real Label'] != df_comparison_rf['Prediction']]
df_different_rf.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsRF_ManLabelSMOTE.xlsx", index=False)

df_different_rf

# prompt: display accuration result for RandomForest

# Assuming y_test contains the true labels and y_pred_rf contains the predictions from your Random Forest model.

accuracy = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy of the Random Forest model: {accuracy}")

"""## Training XGBoost"""

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# 1. Inisialisasi dan training model XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb_model.fit(X_train_smote, y_train_smote)

# 2. Prediksi terhadap data test
y_pred_xgb = xgb_model.predict(X_test)

# 3. Evaluasi performa model
print("=== XGBoost Classification Report ===")
print(classification_report(y_test, y_pred_xgb))

# 4. Visualisasi Confusion Matrix
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=xgb_model.classes_)
disp_xgb.plot()
plt.title("Confusion Matrix - XGBoost")
plt.savefig("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/confusion_matrix_xgb_ManLabelSMOTE.png")
plt.show()

# 5. Cek prediksi yang berbeda dengan label asli
df_comparison_xgb = pd.DataFrame({
    'Text': X_test_text.values,
    'Real Label': y_test.values,
    'Prediction': y_pred_xgb
})

df_different_xgb = df_comparison_xgb[df_comparison_xgb['Real Label'] != df_comparison_xgb['Prediction']]
df_different_xgb.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsXGB_ManLabelSMOTE.xlsx", index=False)

df_different_xgb

# prompt: show accuration result for XGBoost

# Assuming y_test contains the true labels and y_pred_xgb contains the predictions from your XGBoost model.

accuracy = accuracy_score(y_test, y_pred_xgb)
print(f"Accuracy of the XGBoost model: {accuracy}")

"""## Training GradientBoostingClassifier"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# 1. Inisialisasi dan training model Gradient Boosting
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_model.fit(X_train_smote, y_train_smote)

# 2. Prediksi terhadap data test
y_pred_gb = gb_model.predict(X_test)

# 3. Evaluasi performa model
print("=== Gradient Boosting Classification Report ===")
print(classification_report(y_test, y_pred_gb))

# 4. Confusion Matrix
cm_gb = confusion_matrix(y_test, y_pred_gb)
disp_gb = ConfusionMatrixDisplay(confusion_matrix=cm_gb, display_labels=gb_model.classes_)
disp_gb.plot()
plt.title("Confusion Matrix - Gradient Boosting")
plt.savefig("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/confusion_matrix_gb_ManLabelSMOTE.png")
plt.show()

# 5. Tabel perbandingan prediksi vs label asli
df_comparison_gb = pd.DataFrame({
    'Text': X_test_text.values,
    'Real Label': y_test.values,
    'Prediction': y_pred_gb
})

df_different_gb = df_comparison_gb[df_comparison_gb['Real Label'] != df_comparison_gb['Prediction']]
df_different_gb.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsGB_ManLabelSMOTE.xlsx", index=False)

df_different_gb

# prompt: show accuration result for GradientBoostingClassifier

# Assuming y_test contains the true labels and y_pred_gb contains the predictions from your Gradient Boosting model.

accuracy = accuracy_score(y_test, y_pred_gb)
print(f"Accuracy of the Gradient Boosting model: {accuracy}")

"""## Training MLPClassifier"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# 1. Inisialisasi dan training model MLPClassifier
mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam',
                          max_iter=300, random_state=42)
mlp_model.fit(X_train_smote, y_train_smote)

# 2. Prediksi terhadap data test
y_pred_mlp = mlp_model.predict(X_test)

# 3. Evaluasi performa model
print("=== MLPClassifier Classification Report ===")
print(classification_report(y_test, y_pred_mlp))

# 4. Confusion Matrix
cm_mlp = confusion_matrix(y_test, y_pred_mlp)
disp_mlp = ConfusionMatrixDisplay(confusion_matrix=cm_mlp, display_labels=mlp_model.classes_)
disp_mlp.plot()
plt.title("Confusion Matrix - MLPClassifier")
plt.savefig("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/confusion_matrix_mlp_ManLabelSMOTE.png")
plt.show()

# 5. Tabel perbandingan prediksi vs label asli
df_comparison_mlp = pd.DataFrame({
    'Text': X_test_text.values,
    'Real Label': y_test.values,
    'Prediction': y_pred_mlp
})

df_different_mlp = df_comparison_mlp[df_comparison_mlp['Real Label'] != df_comparison_mlp['Prediction']]
df_different_mlp.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsMLP_ManLabelSMOTE.xlsx", index=False)

df_comparison_mlp

# prompt: show accuration score for MLPClassifier

# Assuming y_test contains the true labels and y_pred_mlp contains the predictions from your MLPClassifier model.

accuracy = accuracy_score(y_test, y_pred_mlp)
print(f"Accuracy of the MLPClassifier model: {accuracy}")

"""## Training Deep Neural Network"""

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# 1. Konversi data ke tensor
# X_train_tensor = torch.tensor(X_train_smote, dtype=torch.float32)
X_train_tensor = torch.tensor(X_train_smote.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_smote.values, dtype=torch.float32).view(-1, 1)
# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)

# 2. Definisikan arsitektur model DNN
class SimpleDNN(nn.Module):
    def __init__(self, input_dim):
        super(SimpleDNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 256)
        self.dropout1 = nn.Dropout(0.3)
        self.fc2 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(128, 1)  # Output layer
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.sigmoid(self.fc3(x))
        return x

# 3. Inisialisasi model
input_dim = X_train_tensor.shape[1]
model_dnn = SimpleDNN(input_dim)

# 4. Konfigurasi optimizer dan loss function
criterion = nn.BCELoss()
optimizer = optim.Adam(model_dnn.parameters(), lr=0.001)

# 5. Training loop
epochs = 20
for epoch in range(epochs):
    model_dnn.train()
    optimizer.zero_grad()
    outputs = model_dnn(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

# 6. Predict
model_dnn.eval()
with torch.no_grad():
    y_pred_prob = model_dnn(X_test_tensor).numpy()
    y_pred_dnn = (y_pred_prob > 0.5).astype(int).flatten()

# 7. Evaluasi
print("=== DNN (PyTorch) Classification Report ===")
print(classification_report(y_test, y_pred_dnn))

# 8. Confusion Matrix
cm_dnn = confusion_matrix(y_test, y_pred_dnn)
disp_dnn = ConfusionMatrixDisplay(confusion_matrix=cm_dnn, display_labels=[0, 1])
disp_dnn.plot()
plt.title("Confusion Matrix - DNN (PyTorch)")
plt.savefig("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/confusion_matrix_dnn_pytorch_ManLabelSMOTE.png")
plt.show()

# 9. Tabel perbandingan prediksi vs label asli
df_comparison_dnn = pd.DataFrame({
    'Text': X_test_text.values,
    'Real Label': y_test.values,
    'Prediction': y_pred_dnn
})
df_different_dnn = df_comparison_dnn[df_comparison_dnn['Real Label'] != df_comparison_dnn['Prediction']]
df_different_dnn.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsDNN_PyTorch_ManLabelSMOTE.xlsx", index=False)

df_comparison_dnn

# prompt: show accuration result for DNN

# Assuming y_test contains the true labels and y_pred_dnn contains the predictions from your DNN model.

accuracy = accuracy_score(y_test, y_pred_dnn)
print(f"Accuracy of the DNN model: {accuracy}")

"""## Training KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import pandas as pd

# Inisialisasi dan training model KNN
knn_model = KNeighborsClassifier(n_neighbors=5)  # Default: 5 neighbors
knn_model.fit(X_train_smote, y_train_smote)

# Prediksi terhadap data test
y_pred_knn = knn_model.predict(X_test)

# Evaluasi performa
print("=== KNN Classification Report ===")
print(classification_report(y_test, y_pred_knn))

# Confusion matrix
cm_knn = confusion_matrix(y_test, y_pred_knn)
disp_knn = ConfusionMatrixDisplay(confusion_matrix=cm_knn, display_labels=knn_model.classes_)
disp_knn.plot()
plt.title("Confusion Matrix - KNN")
plt.savefig("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/confusion_matrix_knn_ManLabelSMOTE.png")
plt.show()

# Cek prediksi yang berbeda dengan label asli
df_comparison_knn = pd.DataFrame({
    'Real Label': y_test.values,
    'Prediction': y_pred_knn
})

df_different_knn = df_comparison_knn[df_comparison_knn['Real Label'] != df_comparison_knn['Prediction']]
df_different_knn.to_excel("/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/different_predictionsKNN_ManLabelSMOTE.xlsx", index=False)

df_different_knn

# prompt: show accuration score for KNN

# Assuming y_test contains the true labels and y_pred_knn contains the predictions from your KNN model.

accuracy = accuracy_score(y_test, y_pred_knn)
print(f"Accuracy of the KNN model: {accuracy}")

"""## Tabel perbandingan semua model


"""

import pandas as pd
import matplotlib.pyplot as plt

# Buat DataFrame evaluasi (gantilah nilai-nilainya dengan variabel yang sudah kamu hitung sebelumnya)
df_metrics = pd.DataFrame({
    'Model': ['Logistic Regression', 'Linear SVM', 'Random Forest', 'XGBoost',
              'Gradient Boosting', 'MLPClassifier', 'DNN (PyTorch)', 'KNN'],
    'Accuracy': [0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.66],       # Ganti dengan variabel atau nilai akurat
    'Precision': [0.85, 0.84, 0.84, 0.85, 0.85, 0.84, 0.83, 0.67],      # Contoh nilai precision
    'Recall': [0.87, 0.87, 0.86, 0.86, 0.87, 0.86, 0.87, 0.65],         # Contoh nilai recall
    'F1 Score': [0.86, 0.85, 0.85, 0.86, 0.86, 0.85, 0.85, 0.66]        # Contoh nilai F1-score
})

# Tampilkan tabel evaluasi
print("\n=== Model Evaluation Metrics ===")
print(df_metrics)

# Simpan ke file
# df_metrics.to_csv('/content/model_comparison_metrics.csv', index=False)
# print("\n‚úî Hasil evaluasi model telah disimpan ke 'model_comparison_metrics.csv'")

"""# Bagian 4.c Tabel Perbandingan Hasil Data Imbalanced dan Data Balanced untuk semua model"""

import matplotlib.pyplot as plt

# Data performa model
models = ['LogReg', 'SVM', 'RF', 'XGB', 'GB', 'MLP', 'DNN', 'KNN']

# Data balanced
accuracy_bal = [0.86, 0.86, 0.87, 0.87, 0.86, 0.86, 0.87, 0.66]
precision_bal = [0.86, 0.86, 0.87, 0.87, 0.86, 0.87, 0.87, 0.80]
recall_bal = [0.86, 0.86, 0.87, 0.87, 0.87, 0.86, 0.87, 0.65]
f1_bal = [0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.86, 0.67]

# Data imbalanced
accuracy_imb = [0.85, 0.85, 0.83, 0.86, 0.87, 0.87, 0.88, 0.81]
precision_imb = [0.85, 0.85, 0.85, 0.86, 0.87, 0.87, 0.88, 0.80]
recall_imb = [0.84, 0.85, 0.83, 0.86, 0.87, 0.87, 0.88, 0.80]
f1_imb = [0.85, 0.85, 0.81, 0.86, 0.87, 0.87, 0.88, 0.80]

# Grafik line - Balanced
plt.figure(figsize=(10, 6))
plt.plot(models, accuracy_bal, marker='o', label='Accuracy')
plt.plot(models, precision_bal, marker='o', label='Precision')
plt.plot(models, recall_bal, marker='o', label='Recall')
plt.plot(models, f1_bal, marker='o', label='F1 Score')
plt.title('Model Performance - Balanced Dataset')
plt.xlabel('Model')
plt.ylabel('Score')
plt.ylim(0.6, 0.95)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/model_performance_balanced_line.png')
plt.show()

# Grafik line - Imbalanced
plt.figure(figsize=(10, 6))
plt.plot(models, accuracy_imb, marker='o', label='Accuracy')
plt.plot(models, precision_imb, marker='o', label='Precision')
plt.plot(models, recall_imb, marker='o', label='Recall')
plt.plot(models, f1_imb, marker='o', label='F1 Score')
plt.title('Model Performance - Imbalanced Dataset')
plt.xlabel('Model')
plt.ylabel('Score')
plt.ylim(0.6, 0.95)
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/02_TelU/01_KK-AIS/2022 - PDT Tahap 1 - SentimentAnalysis Quran Playstore App/02b_Experiment03/pic/model_performance_imbalanced_line.png')
plt.show()